{"p2":"Un archivo robots.txt indica a los motores de búsqueda y rastreadores web cuáles partes de tu sitio pueden explorar y cuáles deben evitar. Esta herramienta genera un robots.txt completo permitiéndote especificar directorios bloqueados, archivos prohibidos y ubicación del sitemap XML. Puedes crear reglas específicas para diferentes agentes de usuario (Google, Bing, etc.) o reglas globales aplicables a todos los rastreadores. El archivo generado se descarga listo para subir a la raíz de tu servidor web.","p3":"Los administradores de sitios web protegen secciones privadas bloqueando carpetas de administración de bots. Los propietarios de blogs previenen que los rastreadores indexen archivos duplicados o páginas de etiquetas de bajo valor. Los desarrolladores de comercio electrónico optimizan el rastreo evitando que los bots gasten presupuesto de rastreo en páginas de filtros repetitivos. Los sitios de noticias señalan la ubicación de sitemaps para asegurar descubrimiento rápido de contenido fresco.","p4":"Asegúrate de incluir la ruta del Sitemap XML completa al final de tu archivo robots.txt para mejor descubrimiento. Usa rutas específicas en lugar de bloqueos amplios cuando sea posible para permitir mayor indexación. No incluyas instrucciones conflictivas; si un agente específico tiene reglas, éstas sobrescriben las reglas globales. Verifica tu robots.txt en Google Search Console después de publicar para confirmar que las reglas se interpretan correctamente."}